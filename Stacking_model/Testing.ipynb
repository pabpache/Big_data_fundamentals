{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from submission import base_features_gen_pipeline, gen_meta_features, test_prediction\n",
    "\n",
    "import random\n",
    "rseed = 1024\n",
    "random.seed(rseed)\n",
    "\n",
    "\n",
    "def gen_binary_labels(df):\n",
    "    df = df.withColumn('label_0', (df['label'] == 0).cast(DoubleType()))\n",
    "    df = df.withColumn('label_1', (df['label'] == 1).cast(DoubleType()))\n",
    "    df = df.withColumn('label_2', (df['label'] == 2).cast(DoubleType()))\n",
    "    return df\n",
    "\n",
    "# Create a Spark Session\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"lab3\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Load data\n",
    "train_data = spark.read.load(\"proj2train.csv\", format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"true\")\n",
    "test_data = spark.read.load(\"proj2test.csv\", format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|category|count|\n",
      "+--------+-----+\n",
      "|    FOOD|  901|\n",
      "|     PAS|  542|\n",
      "|    MISC|  798|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.groupBy('category').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|category|count|\n",
      "+--------+-----+\n",
      "|    FOOD|  418|\n",
      "|     PAS|  167|\n",
      "|    MISC|  215|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.groupBy('category').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the pipeline from task 1.1\n",
    "base_features_pipeline = base_features_gen_pipeline()\n",
    "# Fit the pipeline using train_data\n",
    "base_features_pipeline_model = base_features_pipeline.fit(train_data)\n",
    "# Transform the train_data using fitted pipeline\n",
    "training_set = base_features_pipeline_model.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign random groups and binarize the labels\n",
    "training_set = training_set.withColumn('group', (rand(rseed)*5).cast(IntegerType()))\n",
    "training_set = gen_binary_labels(training_set)\n",
    "\n",
    "# define base models\n",
    "nb_0 = NaiveBayes(featuresCol='features', labelCol='label_0', predictionCol='nb_pred_0', probabilityCol='nb_prob_0', rawPredictionCol='nb_raw_0')\n",
    "nb_1 = NaiveBayes(featuresCol='features', labelCol='label_1', predictionCol='nb_pred_1', probabilityCol='nb_prob_1', rawPredictionCol='nb_raw_1')\n",
    "nb_2 = NaiveBayes(featuresCol='features', labelCol='label_2', predictionCol='nb_pred_2', probabilityCol='nb_prob_2', rawPredictionCol='nb_raw_2')\n",
    "svm_0 = LinearSVC(featuresCol='features', labelCol='label_0', predictionCol='svm_pred_0', rawPredictionCol='svm_raw_0')\n",
    "svm_1 = LinearSVC(featuresCol='features', labelCol='label_1', predictionCol='svm_pred_1', rawPredictionCol='svm_raw_1')\n",
    "svm_2 = LinearSVC(featuresCol='features', labelCol='label_2', predictionCol='svm_pred_2', rawPredictionCol='svm_raw_2')\n",
    "\n",
    "# build pipeline to generate predictions from base classifiers, will be used in task 1.3\n",
    "gen_base_pred_pipeline = Pipeline(stages=[nb_0, nb_1, nb_2, svm_0, svm_1, svm_2])\n",
    "gen_base_pred_pipeline_model = gen_base_pred_pipeline.fit(training_set)\n",
    "\n",
    "# task 1.2\n",
    "meta_features = gen_meta_features(training_set, nb_0, nb_1, nb_2, svm_0, svm_1, svm_2)\n",
    "\n",
    "# build onehotencoder and vectorassembler pipeline \n",
    "onehot_encoder = OneHotEncoderEstimator(inputCols=['nb_pred_0', 'nb_pred_1', 'nb_pred_2', 'svm_pred_0', 'svm_pred_1', 'svm_pred_2', 'joint_pred_0', 'joint_pred_1', 'joint_pred_2'], outputCols=['vec{}'.format(i) for i in range(9)])\n",
    "vector_assembler = VectorAssembler(inputCols=['vec{}'.format(i) for i in range(9)], outputCol='meta_features')\n",
    "gen_meta_feature_pipeline = Pipeline(stages=[onehot_encoder, vector_assembler])\n",
    "gen_meta_feature_pipeline_model = gen_meta_feature_pipeline.fit(meta_features)\n",
    "meta_features = gen_meta_feature_pipeline_model.transform(meta_features)\n",
    "\n",
    "# train the meta clasifier\n",
    "lr_model = LogisticRegression(featuresCol='meta_features', labelCol='label', predictionCol='final_prediction', maxIter=20, regParam=1., elasticNetParam=0)\n",
    "meta_classifier = lr_model.fit(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df_sechema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- descript: string (nullable = true)\n",
      "\n",
      "trans_test_df_SCHEMA:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- descript: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n",
      "new_predict_SCHEMA:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- descript: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- nb_raw_0: vector (nullable = true)\n",
      " |-- nb_prob_0: vector (nullable = true)\n",
      " |-- nb_pred_0: double (nullable = false)\n",
      " |-- nb_raw_1: vector (nullable = true)\n",
      " |-- nb_prob_1: vector (nullable = true)\n",
      " |-- nb_pred_1: double (nullable = false)\n",
      " |-- nb_raw_2: vector (nullable = true)\n",
      " |-- nb_prob_2: vector (nullable = true)\n",
      " |-- nb_pred_2: double (nullable = false)\n",
      " |-- svm_raw_0: vector (nullable = true)\n",
      " |-- svm_pred_0: double (nullable = false)\n",
      " |-- svm_raw_1: vector (nullable = true)\n",
      " |-- svm_pred_1: double (nullable = false)\n",
      " |-- svm_raw_2: vector (nullable = true)\n",
      " |-- svm_pred_2: double (nullable = false)\n",
      "\n",
      "new_predict_SCHEMA_NEW:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- descript: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- nb_raw_0: vector (nullable = true)\n",
      " |-- nb_prob_0: vector (nullable = true)\n",
      " |-- nb_pred_0: double (nullable = false)\n",
      " |-- nb_raw_1: vector (nullable = true)\n",
      " |-- nb_prob_1: vector (nullable = true)\n",
      " |-- nb_pred_1: double (nullable = false)\n",
      " |-- nb_raw_2: vector (nullable = true)\n",
      " |-- nb_prob_2: vector (nullable = true)\n",
      " |-- nb_pred_2: double (nullable = false)\n",
      " |-- svm_raw_0: vector (nullable = true)\n",
      " |-- svm_pred_0: double (nullable = false)\n",
      " |-- svm_raw_1: vector (nullable = true)\n",
      " |-- svm_pred_1: double (nullable = false)\n",
      " |-- svm_raw_2: vector (nullable = true)\n",
      " |-- svm_pred_2: double (nullable = false)\n",
      " |-- joint_pred_0: double (nullable = true)\n",
      " |-- joint_pred_1: double (nullable = true)\n",
      " |-- joint_pred_2: double (nullable = true)\n",
      "\n",
      "new_meta_features_SCHEMA:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- descript: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- nb_raw_0: vector (nullable = true)\n",
      " |-- nb_prob_0: vector (nullable = true)\n",
      " |-- nb_pred_0: double (nullable = false)\n",
      " |-- nb_raw_1: vector (nullable = true)\n",
      " |-- nb_prob_1: vector (nullable = true)\n",
      " |-- nb_pred_1: double (nullable = false)\n",
      " |-- nb_raw_2: vector (nullable = true)\n",
      " |-- nb_prob_2: vector (nullable = true)\n",
      " |-- nb_pred_2: double (nullable = false)\n",
      " |-- svm_raw_0: vector (nullable = true)\n",
      " |-- svm_pred_0: double (nullable = false)\n",
      " |-- svm_raw_1: vector (nullable = true)\n",
      " |-- svm_pred_1: double (nullable = false)\n",
      " |-- svm_raw_2: vector (nullable = true)\n",
      " |-- svm_pred_2: double (nullable = false)\n",
      " |-- joint_pred_0: double (nullable = true)\n",
      " |-- joint_pred_1: double (nullable = true)\n",
      " |-- joint_pred_2: double (nullable = true)\n",
      " |-- vec4: vector (nullable = true)\n",
      " |-- vec7: vector (nullable = true)\n",
      " |-- vec0: vector (nullable = true)\n",
      " |-- vec1: vector (nullable = true)\n",
      " |-- vec6: vector (nullable = true)\n",
      " |-- vec2: vector (nullable = true)\n",
      " |-- vec5: vector (nullable = true)\n",
      " |-- vec3: vector (nullable = true)\n",
      " |-- vec8: vector (nullable = true)\n",
      " |-- meta_features: vector (nullable = true)\n",
      "\n",
      "final_predictions_SCHEMA:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- descript: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- nb_raw_0: vector (nullable = true)\n",
      " |-- nb_prob_0: vector (nullable = true)\n",
      " |-- nb_pred_0: double (nullable = false)\n",
      " |-- nb_raw_1: vector (nullable = true)\n",
      " |-- nb_prob_1: vector (nullable = true)\n",
      " |-- nb_pred_1: double (nullable = false)\n",
      " |-- nb_raw_2: vector (nullable = true)\n",
      " |-- nb_prob_2: vector (nullable = true)\n",
      " |-- nb_pred_2: double (nullable = false)\n",
      " |-- svm_raw_0: vector (nullable = true)\n",
      " |-- svm_pred_0: double (nullable = false)\n",
      " |-- svm_raw_1: vector (nullable = true)\n",
      " |-- svm_pred_1: double (nullable = false)\n",
      " |-- svm_raw_2: vector (nullable = true)\n",
      " |-- svm_pred_2: double (nullable = false)\n",
      " |-- joint_pred_0: double (nullable = true)\n",
      " |-- joint_pred_1: double (nullable = true)\n",
      " |-- joint_pred_2: double (nullable = true)\n",
      " |-- vec4: vector (nullable = true)\n",
      " |-- vec7: vector (nullable = true)\n",
      " |-- vec0: vector (nullable = true)\n",
      " |-- vec1: vector (nullable = true)\n",
      " |-- vec6: vector (nullable = true)\n",
      " |-- vec2: vector (nullable = true)\n",
      " |-- vec5: vector (nullable = true)\n",
      " |-- vec3: vector (nullable = true)\n",
      " |-- vec8: vector (nullable = true)\n",
      " |-- meta_features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- final_prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task 1.3\n",
    "pred_test = test_prediction(test_data, base_features_pipeline_model, gen_base_pred_pipeline_model, gen_meta_feature_pipeline_model, meta_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+------------+\n",
      "| id|nb_pred_0|svm_pred_0|joint_pred_0|\n",
      "+---+---------+----------+------------+\n",
      "|  3|      0.0|       0.0|         0.0|\n",
      "|  5|      1.0|       1.0|         3.0|\n",
      "| 14|      0.0|       0.0|         0.0|\n",
      "| 21|      1.0|       0.0|         2.0|\n",
      "| 22|      0.0|       0.0|         0.0|\n",
      "+---+---------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_features.select('id','nb_pred_0','svm_pred_0','joint_pred_0').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+------------+\n",
      "| id|nb_pred_1|svm_pred_1|joint_pred_1|\n",
      "+---+---------+----------+------------+\n",
      "|  3|      1.0|       0.0|         2.0|\n",
      "|  5|      0.0|       0.0|         0.0|\n",
      "| 14|      1.0|       1.0|         3.0|\n",
      "| 21|      0.0|       0.0|         0.0|\n",
      "| 22|      1.0|       1.0|         3.0|\n",
      "+---+---------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_features.select('id','nb_pred_1','svm_pred_1','joint_pred_1').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+------------+\n",
      "| id|nb_pred_2|svm_pred_2|joint_pred_2|\n",
      "+---+---------+----------+------------+\n",
      "|  3|      0.0|       0.0|         0.0|\n",
      "|  5|      0.0|       0.0|         0.0|\n",
      "| 14|      0.0|       0.0|         0.0|\n",
      "| 21|      0.0|       1.0|         1.0|\n",
      "| 22|      0.0|       0.0|         0.0|\n",
      "+---+---------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_features.select('id','nb_pred_2','svm_pred_2','joint_pred_2').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2241"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_features.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2241"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=training_set.filter(training_set['group']==1).withColumn('extra_1',lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            features|extra_1|\n",
      "+--------------------+-------+\n",
      "|(5421,[0,7,47,49,...|      0|\n",
      "|(5421,[0,3,4,14,7...|      0|\n",
      "|(5421,[0,1,4,8,13...|      0|\n",
      "|(5421,[2,10,171,5...|      0|\n",
      "|(5421,[0,3,11,12,...|      0|\n",
      "|(5421,[3,6,10,19,...|      0|\n",
      "|(5421,[0,21,25,43...|      0|\n",
      "|(5421,[7,14,28,72...|      0|\n",
      "|(5421,[1,2,3,18,2...|      0|\n",
      "|(5421,[0,1,2,4,7,...|      0|\n",
      "|(5421,[42,66,1177...|      0|\n",
      "|(5421,[0,2,3,4,10...|      0|\n",
      "|(5421,[0,1,6,10,1...|      0|\n",
      "|(5421,[1,2,5,16,2...|      0|\n",
      "|(5421,[0,1,12,16,...|      0|\n",
      "|(5421,[6,17,22,25...|      0|\n",
      "|(5421,[2,8,13,14,...|      0|\n",
      "|(5421,[0,1,2,4,8,...|      0|\n",
      "|(5421,[5,94,163,2...|      0|\n",
      "|(5421,[0,1,2,3,5,...|      0|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss['features','extra_1'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt=[]\n",
    "for i in range(439):\n",
    "    tt.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ss2=training_set.filter(training_set['group']==1).withColumn('extra_1',lit(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/COMP9313/lib/python3.6/site-packages/pyspark/sql/session.py:366: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1\n",
      "+---+-----+-----+\n",
      "|key|val11|val12|\n",
      "+---+-----+-----+\n",
      "|abc|  1.1|  1.2|\n",
      "|def|  3.0|  3.4|\n",
      "+---+-----+-----+\n",
      "\n",
      "df2\n",
      "+---+-----+-----+\n",
      "|key|val21|val22|\n",
      "+---+-----+-----+\n",
      "|abc|  2.1|  2.2|\n",
      "|xyz|  3.1|  3.2|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName('joins_example').getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "dataset1 = [\n",
    "\n",
    "  {\n",
    "\n",
    "  'key' : 'abc',\n",
    "\n",
    "  'val11' : 1.1,\n",
    "\n",
    "  'val12' : 1.2\n",
    "\n",
    "  },\n",
    "\n",
    "  {\n",
    "\n",
    "  'key' : 'def',\n",
    "\n",
    "  'val11' : 3.0,\n",
    "\n",
    "  'val12' : 3.4\n",
    "\n",
    "  }\n",
    "\n",
    "]\n",
    "\n",
    "dataset2 = [\n",
    "\n",
    "  {\n",
    "\n",
    "  'key' : 'abc',\n",
    "\n",
    "  'val21' : 2.1,\n",
    "\n",
    "  'val22' : 2.2\n",
    "\n",
    "  },\n",
    "\n",
    "  {\n",
    "\n",
    "  'key' : 'xyz',\n",
    "\n",
    "  'val21' : 3.1,\n",
    "\n",
    "  'val22' : 3.2\n",
    "\n",
    "  }\n",
    "\n",
    "]\n",
    "\n",
    "rdd1 = sc.parallelize(dataset1)\n",
    "\n",
    "df1 = spark.createDataFrame(rdd1)\n",
    "\n",
    "print('df1')\n",
    "\n",
    "df1.show()\n",
    "\n",
    "rdd2 = sc.parallelize(dataset2)\n",
    "\n",
    "df2 = spark.createDataFrame(rdd2)\n",
    "\n",
    "print('df2')\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "|key|val11|val12|\n",
      "+---+-----+-----+\n",
      "|abc|  1.1|  1.2|\n",
      "|def|  3.0|  3.4|\n",
      "|abc|  2.1|  2.2|\n",
      "|xyz|  3.1|  3.2|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df1.union(df2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "|key|val11|val12|\n",
      "+---+-----+-----+\n",
      "|abc|  1.1|  1.2|\n",
      "|def|  3.0|  3.4|\n",
      "|abc|  2.1|  2.2|\n",
      "|xyz|  3.1|  3.2|\n",
      "|abc|  2.1|  2.2|\n",
      "|xyz|  3.1|  3.2|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_u=df.union(df2)\n",
    "df_u.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---+-----+-----+\n",
      "|key|val11|val12|key|val21|val22|\n",
      "+---+-----+-----+---+-----+-----+\n",
      "|abc|  1.1|  1.2|abc|  2.1|  2.2|\n",
      "+---+-----+-----+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df1.join(df2, df1.key ==df2.key, how='inner')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+-----+\n",
      "|key|val11|val12|val21|val22|\n",
      "+---+-----+-----+-----+-----+\n",
      "|abc|  1.1|  1.2|  2.1|  2.2|\n",
      "+---+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = df1.join(df2, on=['key'], how='inner')\n",
    "\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+-----+\n",
      "|key|val11|val12|val21|val22|\n",
      "+---+-----+-----+-----+-----+\n",
      "|abc|  1.1|  1.2|  2.1|  2.2|\n",
      "|def|  3.0|  3.4| null| null|\n",
      "+---+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2= df1.join(df2, on=['key'], how='left')\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = [\n",
    "\n",
    "  {\n",
    "\n",
    "  'key' : 'def',\n",
    "\n",
    "  'val11' : 3.0,\n",
    "\n",
    "  'val12' : 3.4, 'val21':5, 'val22':6\n",
    "\n",
    "  }\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+-----+\n",
      "|key|val11|val12|val21|val22|\n",
      "+---+-----+-----+-----+-----+\n",
      "|def|  3.0|  3.4|    5|    6|\n",
      "+---+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd3 = sc.parallelize(dataset3)\n",
    "\n",
    "df3 = spark.createDataFrame(rdd3)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+-----+---+-----+-----+-----+-----+\n",
      "|key|val11|val12|val21|val22|key|val11|val12|val21|val22|\n",
      "+---+-----+-----+-----+-----+---+-----+-----+-----+-----+\n",
      "|def|  3.0|  3.4| null| null|def|  3.0|  3.4|    5|    6|\n",
      "+---+-----+-----+-----+-----+---+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_4 = df_2.join(df3, [df_2.key==df3.key, df_2.val11==df3.val11, df_2.val12==df_2.val12],how='inner')\n",
    "df_4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = df_2.join(df3, df_2.key==df3.key and df_2.val11=df3.val11 and df_2.val12==df_2.val12,how='inner')\n",
    "df_4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "|key|val21|val22|\n",
      "+---+-----+-----+\n",
      "|abc|  2.1|  2.2|\n",
      "|xyz|  3.1|  3.2|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+-----+\n",
      "|key|val11|val12|val21|val22|\n",
      "+---+-----+-----+-----+-----+\n",
      "|abc|  1.1|  1.2|  2.1|  2.2|\n",
      "|def|  3.0|  3.4| null| null|\n",
      "+---+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'double' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4ff3b3f6c193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'double' is not defined"
     ]
    }
   ],
   "source": [
    "w=double(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(2==2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.PipelineModel"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(base_features_pipeline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+--------------------+--------------------+-----+\n",
      "| id|category|            descript|               words|            features|label|\n",
      "+---+--------+--------------------+--------------------+--------------------+-----+\n",
      "|  0|    MISC|I've been there t...|[i've, been, ther...|(5421,[1,18,31,39...|  1.0|\n",
      "|  1|    FOOD|Stay away from th...|[stay, away, from...|(5421,[0,1,15,20,...|  0.0|\n",
      "|  2|    FOOD|Wow over 100 beer...|[wow, over, 100, ...|(5421,[3,109,556,...|  0.0|\n",
      "+---+--------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_features_pipeline_model.transform(train_data).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP9313",
   "language": "python",
   "name": "comp9313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
